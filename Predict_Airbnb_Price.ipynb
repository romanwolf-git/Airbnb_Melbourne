{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9975724e",
   "metadata": {},
   "source": [
    "# Predict the Price of an Airbnb\n",
    "Before training a gradient boosting model or any machine learning model, it's important to preprocess the data to ensure it is in a suitable format and contains meaningful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82288492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b0fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_listings=pd.read_csv('listings.csv')\n",
    "print(f'df_listings shape: {df_listings.shape}')\n",
    "print(df_listings.info())\n",
    "df_listings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d965d4",
   "metadata": {},
   "source": [
    "## Heatmaps for Correlation Matrices for Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d67e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "# Create a cmap which we can use in the entire notebook\n",
    "\n",
    "# Define the starting and ending colors as from Airbnb\n",
    "start_color = '#00A699'\n",
    "end_color = '#FF5A5F'\n",
    "\n",
    "# Create a list of color stops\n",
    "color_stops = [0.0, 1.0]\n",
    "\n",
    "# Create a list of colors corresponding to the color stops\n",
    "colors = [start_color, end_color]\n",
    "\n",
    "# Create the colormap using LinearSegmentedColormap\n",
    "cmap = LinearSegmentedColormap.from_list('custom_colormap', list(zip(color_stops, colors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3faf8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for all numeric review columns\n",
    "corr_matrix = df_listings.filter(like='review').select_dtypes(exclude='object').corr()\n",
    "sn.heatmap(corr_matrix, cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51f8eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for the remaining numeric columns\n",
    "corr_matrix = df_listings.filter(regex=r'^(?!.*review).*$', axis=1).select_dtypes(exclude='object').corr()\n",
    "sn.heatmap(corr_matrix, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f92f9",
   "metadata": {},
   "source": [
    "The plot shows that several features are correlated and should be dealt with. We will do this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb497ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "review_cols = df_listings.filter(like='review').select_dtypes(exclude='object').columns\n",
    "num_cols = len(review_cols)\n",
    "\n",
    "num_rows = math.ceil(num_cols / 5)  # Update the number of rows and subplot grid\n",
    "fig, ax = plt.subplots(num_rows, 5, figsize=(15, num_rows * 3))  # Adjust the figure size\n",
    "\n",
    "for i, col in enumerate(review_cols):\n",
    "    ax.flat[i].hist(df_listings[col], bins=50)\n",
    "    title = col.replace('_', ' ').title()\n",
    "    ax.flat[i].set_title(title)\n",
    "\n",
    "# Remove empty subplots\n",
    "if num_cols < num_rows * 5:  # Update the condition for removing empty subplots\n",
    "    for j in range(num_cols, num_rows * 5):\n",
    "        fig.delaxes(ax.flat[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9224e1f1",
   "metadata": {},
   "source": [
    "The plots show, that most reviews range between 4 and 5 and the respective feature is skewed to the left. However, number of reviews are usually skewed to the right. Neither of the features are normally distributed. Applying log could help normalize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e373f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns of specific data types\n",
    "numeric_columns = df_listings.select_dtypes(include=['int', 'float']).columns\n",
    "categorical_columns = df_listings.select_dtypes(include=['object']).columns\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d02439",
   "metadata": {},
   "source": [
    "The string columns contain:\n",
    "* several categorical columns that need to be encoded\n",
    "* several date columns: some that could be used like \"host_since\" and others like \"calendar_last_scaped\" that are less useful\n",
    "* several boolean columns that need to be converted from t/f to 1/0 like \"instant_bookable\", e.g. using np.where\n",
    "* multiple columns with percentages that need to be converted to floats like host_response_rate\n",
    "* column \"amenities\" contains lists\n",
    "# 1. Data Cleaning\n",
    "## 1.1 Handling Missing Values\n",
    "Check for missing values in the dataset and decide how to handle them. You can either remove rows with missing values, impute the missing values with appropriate methods (e.g., mean, median, or most frequent value), or use advanced imputation techniques if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with all Nan-values\n",
    "df_listings = df_listings.dropna(how='all', axis=1)\n",
    "# Drop rows with Nan-values in target column price\n",
    "df_listings = df_listings.dropna(subset='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea60d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df_listings.isna().sum().sort_values(ascending=False).head(32).to_frame()\n",
    "df_na.rename(columns = {0:'abs'}, inplace = True)\n",
    "df_na['rel'] = round(df_na['abs']/len(df_listings)*100, 2)\n",
    "df_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934a8d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop several columns of irrelevant information\n",
    "df_listings.drop(axis=1, columns=['description',\n",
    "                                  'host_about',\n",
    "                                  'host_neighbourhood',\n",
    "                                  'host_name',\n",
    "                                  'host_picture_url',\n",
    "                                  'host_thumbnail_url',\n",
    "                                  'name',\n",
    "                                  'neighborhood_overview'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb1c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df_listings.isna().sum().sort_values(ascending=False).head(32).to_frame()\n",
    "df_na.rename(columns = {0:'abs'}, inplace = True)\n",
    "df_na['rel'] = round(df_na['abs']/len(df_listings)*100, 2)\n",
    "df_na"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf7602",
   "metadata": {},
   "source": [
    "There are 32 columns with missing values:\n",
    "* some don't contain valuable information and can be dropped\n",
    "* some like review_scores should be averaged\n",
    "\n",
    "Since some tree based algorithms are not sensitive to missing values lets keep one dataset with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aaef4e",
   "metadata": {},
   "source": [
    "## 1.2 Removing duplicates\n",
    "Identifying and removing duplicate records to avoid bias and ensure data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85f73ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any duplicated rows?\n",
    "df_listings.duplicated().sum()\n",
    "df_listings_tree.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96745112",
   "metadata": {},
   "source": [
    "## 1.3 Fixing inconsistencies\n",
    "Correcting inconsistencies in the data, such as formatting errors, spelling variations, or data entry mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc35dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df_listings, df_listings_tree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c9aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_list)):\n",
    "    df = df_list[i]\n",
    "    \n",
    "    # Clean neighbourhood column\n",
    "    # Convert the 'neighbourhood' column to string type\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str)\n",
    "\n",
    "    # Apply string operations to clean and format the 'neighbourhood' column\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.strip().str.title()\n",
    "\n",
    "    # Define the patterns to be replaced\n",
    "    pattern_remove = r', Victoria, Australia|, Vi, Australia|, Vic, Australia|, Australia|, Victoria, Au| Vic| - Flagstaff |'\n",
    "    pattern_swap_north = r'(\\bNorth\\b) (\\w+)'\n",
    "    pattern_swap_east = r'(\\bEast\\b) (\\w+)'\n",
    "    pattern_swap_south = r'(\\bSouth\\b) (\\w+)'\n",
    "    pattern_swap_west = r'(\\bWest\\b) (\\w+)'\n",
    "    pattern_swap_upper = r'(\\bUpper\\b) (\\w+)'\n",
    "    pattern_st_kilda = r'(?i)(st[.\\s]*kilda|saint[.\\s]*kilda)'\n",
    "\n",
    "    # Apply regex substitution to the 'neighbourhood' column\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace(pattern_remove, '', regex=True)\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace(pattern_swap_north, r'\\2 \\1', regex=True)\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace(pattern_swap_east, r'\\2 \\1', regex=True)\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace(pattern_swap_south, r'\\2 \\1', regex=True)\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace(pattern_swap_west, r'\\2 \\1', regex=True)\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace(pattern_swap_upper, r'\\2 \\1', regex=True)\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace(pattern_st_kilda, 'St. Kilda', regex=True)\n",
    "\n",
    "\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace(' Melbourne ', '')\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace('，Melbourne', '')\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace(', Melbourne', '')\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace(r'\\s\\(Melbourne\\)$|\\sMelbourne\\s', '', regex=True)\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace(r',\\s*Melbourne\\s*,', '', regex=True)\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace(r'\\bMelbourne,\\s*', '', regex=True)\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace(r'\\s*/\\s*Melbourne\\b', '', regex=True)\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace(' Melbourne', '')\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace('墨尔本, 维多利亚', 'Melbourne')\n",
    "\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace('St EaSt. Kilda', 'St. Kilda East')\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace('St. KildaWest', 'St. Kilda West')\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace('^Kilda', 'St. Kilda', regex=True)\n",
    "\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace(' City', '')\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace(r'\\(.*?\\)', '', regex=True)\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace('St Albans', 'St. Albans')\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace('Yarra Valley, Yarra Glen, Healesville', 'Yarra Glen')\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace('Yellingbo, Yarra Valley', 'Yellingbo')\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace('Mt', 'Mount')\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace('Healesville, Mount Toolebewong', 'Healesville')\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.replace('Chum Creek/Healesville', 'Chum Creek')\n",
    "\n",
    "    df['neighbourhood'] = df['neighbourhood'].str.strip()\n",
    "    df.drop(axis=1, columns=['neighbourhood_cleansed'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e212a85d",
   "metadata": {},
   "source": [
    "## 1.4 Addressing data format issues\n",
    "Converting data types, dealing with units of measurement, or handling formatting inconsistencies.\n",
    "### Extract floats from price column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8bd9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_list)):\n",
    "    df = df_list[i]\n",
    "    \n",
    "    df['price'] = df['price'].str.replace('$', '').str.replace(',', '').astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8236e5ed",
   "metadata": {},
   "source": [
    "### Convert columns with percentages to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f22bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_list)):\n",
    "    df = df_list[i]\n",
    "    \n",
    "    # Select columns with object dtype\n",
    "    cols_str = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Iterate over the selected columns\n",
    "    for col in cols_str:\n",
    "        # Check if any value in the column matches the percentage pattern\n",
    "        try:\n",
    "            if df[col].str.match('[0-9]*\\%$').any():\n",
    "\n",
    "                # Extract the numeric part and convert to float\n",
    "                df[col] = df[col].str.extract('(.*)\\%')[0].astype(float)\n",
    "\n",
    "                # Divide the values by 100 to convert to decimal and round\n",
    "                df[col] = round(df[col] / 100, 2)\n",
    "\n",
    "        # Handle AttributeError for columns containing NA values\n",
    "        except AttributeError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d448650",
   "metadata": {},
   "source": [
    "## 1.5 Handling outliers\n",
    "Identifying and handling outliers, which are extreme values that deviate significantly from the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7d002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "threshold = 3\n",
    "\n",
    "for i in range(len(df_list)):\n",
    "    df = df_list[i]\n",
    "    \n",
    "    # Select non-binary and non-object columns\n",
    "    columns_filtered = df.select_dtypes(exclude=['object']).columns\n",
    "    binary_columns = [col for col in columns_filtered if set(df[col].unique()) == {0, 1}]\n",
    "    final_columns = list(set(columns_filtered) - set(binary_columns))\n",
    "\n",
    "    # Drop rows with zscore > threshold\n",
    "    zscore_3_indices = set()\n",
    "    for col in final_columns:\n",
    "        col_zscores = zscore(df[col])\n",
    "        zscore_3_indices.update(df[col_zscores > threshold].index)\n",
    "    \n",
    "    df_list[i] = df_list[i].drop(zscore_3_indices, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5655f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(i, df_list[i].shape) for i in range(len(df_list))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec664471",
   "metadata": {},
   "source": [
    "## 2. Feature Selection\n",
    "Analyze the available features and select the relevant ones for prediction. Remove any irrelevant or redundant features that do not contribute significantly to the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_list)):\n",
    "    df = df_list[i]\n",
    "    \n",
    "    # Drop URL columns\n",
    "    cols_url = [col for col in df.columns if 'url' in col]\n",
    "    df.drop(axis=1, columns=cols_url, inplace=True)\n",
    "    # Drop scraping columns\n",
    "    cols_scrape = [col for col in df.columns if 'scrape' in col]\n",
    "    df.drop(axis=1, columns= cols_scrape, inplace=True)\n",
    "    # Drop columns without irrelevant information\n",
    "    df.drop(axis=1, inplace=True,\n",
    "            columns=['host_id', 'host_location',\n",
    "                     'id',\n",
    "                     'longitude', 'latitude',\n",
    "                     'source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7ea4a8",
   "metadata": {},
   "source": [
    "## 3. Encoding Categorical Variables\n",
    "Convert categorical variables into numerical representations so that they can be used in the model. This can be done through one-hot encoding, label encoding, or ordinal encoding, depending on the nature of the categorical variables and the requirements of the model.\n",
    "### Correctly encode boolean columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05a1de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_boolean = ['has_availability', 'host_is_superhost', 'host_has_profile_pic', 'host_identity_verified',\n",
    "                'instant_bookable']\n",
    "\n",
    "for i in range(len(df_list)):\n",
    "    df = df_list[i]\n",
    "    \n",
    "    for col in cols_boolean:\n",
    "        # Encode boolean columns with t/f to 1/0 \n",
    "        df[col] = np.where(df[col] == 't', 1,\n",
    "                           np.where(df[col] == 'f', 0, np.nan))\n",
    "\n",
    "        # Check if there are any NA values in the column\n",
    "        if df[col].isna().any():\n",
    "            # Convert the column to float if NA values are present\n",
    "            df[col] = df[col].astype(float)\n",
    "        else:\n",
    "            # Convert the column to int8 if there are no NA values\n",
    "            df[col] = df[col].astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2604c9bd",
   "metadata": {},
   "source": [
    "### Encode host_verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed22fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regex to make a hard distinction between email and work_email\n",
    "pattern = r\"(?<!_)email\"\n",
    "# Define the lambda function to create the new column\n",
    "lambda_email = lambda x: 1 if isinstance(x, list) and any(re.search(pattern, item) for item in x) else 0\n",
    "lambda_work_email = lambda x: 1 if isinstance(x, list) and 'work_email' in x else 0\n",
    "lambda_phone = lambda x: 1 if isinstance(x, list) and 'phone' in x else 0\n",
    "\n",
    "\n",
    "for i in range(len(df_list)):\n",
    "    df = df_list[i]\n",
    "    \n",
    "    # Apply the lambda functions to the desired column\n",
    "    df['host_verifications_email'] = df['host_verifications'].apply(lambda_email)\n",
    "    df['host_verifications_work_email'] = df['host_verifications'].apply(lambda_work_email)\n",
    "    df['host_verifications_phone'] = df['host_verifications'].apply(lambda_phone)\n",
    "\n",
    "    # Drop host_verifications column\n",
    "    df.drop(axis=1, columns='host_verifications', inplace=True)\n",
    "    \n",
    "    df_list[i] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf598c8",
   "metadata": {},
   "source": [
    "### One-hot encode remaining categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Define the columns to be one-hot encoded\n",
    "cols_categorical = ['host_response_time', 'room_type', 'property_type', 'neighbourhood']\n",
    "\n",
    "for i in range(len(df_list)):\n",
    "    # Preprocess the categorical columns\n",
    "    df = df_list[i]\n",
    "    for col in cols_categorical:\n",
    "        df[col] = df[col].fillna('').str.lower().str.replace(' ', '_')\n",
    "\n",
    "    # Create an instance of OneHotEncoder\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "    # Fit and transform the selected columns\n",
    "    encoded_data = encoder.fit_transform(df[cols_categorical])\n",
    "\n",
    "    # Create a DataFrame with the encoded data\n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(cols_categorical))\n",
    "    df_encoded.rename(columns={'host_response_time_': 'host_response_time_na'}, inplace=True)\n",
    "\n",
    "    # Reset indices to avoid problems when concatenating\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df_encoded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Concatenate the encoded DataFrame with the original DataFrame\n",
    "    df = pd.concat([df, df_encoded], axis=1)\n",
    "\n",
    "    # Drop the original categorical columns\n",
    "    df.drop(columns=cols_categorical, inplace=True)\n",
    "\n",
    "    # Assign the modified DataFrame back to the list\n",
    "    df_list[i] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb1789d",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering\n",
    "Create new features that might be more informative or relevant for the prediction task. For example, you could calculate additional derived features from existing ones or create interaction terms between variables.\n",
    "### Make date columns usable by calculating durations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a287bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Date columns .to_datetime() and get duration in days until data retrieval\n",
    "# End date from http://insideairbnb.com for Melbourne, Australia\n",
    "end_date = datetime.strptime('13-03-2023', '%d-%m-%Y')\n",
    "\n",
    "# Convert date columns to datetime and calculate the days passed from the end date\n",
    "date_columns = ['host_since', 'first_review', 'last_review']\n",
    "\n",
    "for i in range(len(df_list)):\n",
    "    df = df_list[i]\n",
    "    \n",
    "    for col in date_columns:\n",
    "        # Convert the column to datetime format\n",
    "        df[col] = pd.to_datetime(df[col], format='%Y-%m-%d')\n",
    "        # Calculate the number of days passed from the end date\n",
    "        df[col] = (end_date - df[col]).dt.days\n",
    "        \n",
    "    df_list[i] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd1b54",
   "metadata": {},
   "source": [
    "### Extract floats from bathrooms_text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee93f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression pattern to extract float numbers\n",
    "pattern = r'(\\d+(?:\\.\\d+)?)'\n",
    "\n",
    "for i in range(len(df_list)):\n",
    "    df = df_list[i]\n",
    "    \n",
    "    df['bathrooms'] = df['bathrooms_text'].str.extract(pattern).astype(float)\n",
    "\n",
    "    # Map remaining text-based values\n",
    "    mapping = {'Shared half-bath': 0.5, 'Half-bath': 0.5, 'Private half-bath': 0.5}\n",
    "    df.loc[df['bathrooms'].isna(), 'bathrooms'] = df.loc[df['bathrooms'].isna(), 'bathrooms_text'].map(mapping)\n",
    "\n",
    "    # Drop bathrooms_text column\n",
    "    df.drop(axis=1, columns=['bathrooms_text'], inplace=True)\n",
    "    \n",
    "    # Assign the modified DataFrame back to df_list[i]\n",
    "    df_list[i] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2273b557",
   "metadata": {},
   "source": [
    "### Extract relevant amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684242f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# create a defaultdict to count the occurrences of amenities within df_listings['amenities']\n",
    "dict_amenities = defaultdict(int)\n",
    "\n",
    "for row in df_listings['amenities']:\n",
    "    if pd.notnull(row):  # Check for NaN values\n",
    "        for item in re.findall(r'\"(.*?)\"', row):\n",
    "            dict_amenities[item.lower().strip()] += 1\n",
    "        \n",
    "# Sort dict_amenities by values in ascending order\n",
    "dict_amenities = dict(sorted(dict_amenities.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# print first 5 key-value pairs of dict_amenities\n",
    "print('dict_amenities')\n",
    "count = 0\n",
    "for key, value in dict_amenities.items():\n",
    "    if count >= 1:\n",
    "        print(f'{key}: {value}')\n",
    "    count += 1\n",
    "    if count == 11:\n",
    "        break\n",
    "print()\n",
    "\n",
    "# List all amenities that occur at least in 0.5% of all Listings, thus capturing 99.5%\n",
    "amenity_types = [key for key, value in dict_amenities.items() if value>len(df_listings)*.005]\n",
    "amenity_types = sorted(amenity_types)\n",
    "\n",
    "print('amenity_types:', amenity_types[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cec6fd",
   "metadata": {},
   "source": [
    "The types of amenities sometimes contain unnecessary information that can be summarised such as:\n",
    "- 'coffee maker',\n",
    "- 'coffee maker: espresso machine',\n",
    "- 'coffee maker: french press',\n",
    "- 'coffee maker: nespresso',\n",
    "- 'coffee maker: pour-over coffee',\n",
    "\n",
    "simply as 'coffee maker'. Different expressions for the same meaning can be accounted for using regular expressions such as r'\\b(ac|air conditioning)\\b' which matches AC and air conditioning. The regular expressions can also be placed in amenity_types before finally assigning the amenities using the assign_amenity() function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e27258c",
   "metadata": {},
   "source": [
    "### Write function to get synonyms for amenities\n",
    "So that amenities with same meaning are are put together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa1b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_noun_synonyms(word):\n",
    "# function to find synonyms for amenities\n",
    "    synonyms = {word}\n",
    "    word = word.replace(' ', '_')\n",
    "    for synset in wordnet.synsets(word, pos=wordnet.NOUN):\n",
    "        for lemma in synset.lemmas():\n",
    "            lemma_name = lemma.name()\n",
    "            if not any(c.isdigit() or c == '_' for c in lemma_name):\n",
    "                synonyms.add(lemma_name)\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c504dd90",
   "metadata": {},
   "source": [
    "### Assign amenities using a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2eb49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_amenity(df):\n",
    "    # function to assign amenities\n",
    "    \n",
    "    # Condensed amenity_types list according to frequency of dict_amenities\n",
    "    # Captures and summarizes 99.5% of the occurring data\n",
    "    # Partly contains regex to capture different writings or similar meanings\n",
    "    \n",
    "    amenity_types = [\n",
    "        r'\\b(ac|air conditioning)\\b',\n",
    "        'baby bath', 'baby safety gates', 'babysitter recommendations', 'backyard', 'baking sheet',\n",
    "        'barbecue utensils', 'bathtub', 'bbq', 'beach access', 'beach essentials', 'bed linens',\n",
    "        'bidet', 'bikes', 'blender', 'bluetooth sound system', 'board games', 'body soap',\n",
    "        'books and reading material', 'bread maker', 'breakfast', 'building staff',\n",
    "        'carbon monoxide alarm', 'ceiling fan', 'central air conditioning', 'changing table',\n",
    "        r'children.*books and toys', r'children.*dinnerware',\n",
    "        'cleaning available during stay', 'cleaning products', 'clothing storage', 'coffee maker',\n",
    "        'conditioner', 'cooking basics', 'crib',\n",
    "        'dedicated workspace', 'dining table', 'dishes and silverware', 'dishwasher',\n",
    "        'drying rack for clothing',\n",
    "        'elevator', 'essentials', 'ethernet connection', 'ev charger', 'exercise equipment',\n",
    "        'extra pillows and blankets',\n",
    "        'fire extinguisher', 'fire pit', 'fireplace', 'first aid kit',\n",
    "        r'(free(.*)parking|free(.*)garage)', r'\\b(?<!paid\\s)(free\\s)?dryer\\b',\n",
    "        r'\\b(?<!paid\\s)(free\\s)?washer\\b',\n",
    "        'game console', 'gym',\n",
    "        'hair dryer', 'hammock', 'hangers', 'heating', 'high chair', 'host greets you', 'hot tub',\n",
    "        'hot water', 'hot water kettle',\n",
    "        'iron',\n",
    "        'keypad', 'kitchen',\n",
    "        'lake access', 'laundromat nearby', 'lock on bedroom door', 'lockbox',\n",
    "        'long term stays allowed', 'luggage dropoff allowed',\n",
    "        'microwave', 'mini fridge', 'mosquito net',\n",
    "        'outdoor dining area', 'outdoor furniture', 'outdoor shower', 'outlet covers', 'oven',\n",
    "        'paid dryer', r'paid(.*)parking', 'paid washer', 'patio or balcony', 'pets allowed',\n",
    "        'piano', 'ping pong table', 'pool', 'portable fans', 'portable heater',\n",
    "        'private entrance', 'private living room',\n",
    "        'record player', 'refrigerator', 'resort access', 'rice maker', 'room-darkening shades',\n",
    "        'safe', 'sauna', 'security cameras on property', 'self check-in', 'shampoo',\n",
    "        'shower gel', 'single level home', 'smart lock', 'smoke alarm', 'smoking allowed',\n",
    "        'sound system', 'stove', 'sun loungers',\n",
    "        'toaster', 'trash compactor', 'tv',\n",
    "        'view',\n",
    "        'waterfront', 'wifi', 'window guards', 'wine glasses'\n",
    "    ]\n",
    "\n",
    "    # create empty array to be filled and then concatenated to df\n",
    "    amenity_results = np.zeros((len(df), len(amenity_types)), dtype=np.int8)\n",
    "\n",
    "    # Iterate over amenity_types to create a list with column names to assign to df_amenities\n",
    "    cols_amenities = []\n",
    "    for i, amenity in enumerate(amenity_types):\n",
    "        if amenity == r'\\b(ac|air conditioning)\\b':\n",
    "            col_name = 'amenity_air_conditioning'\n",
    "        elif amenity == r'children.*books and toys':\n",
    "            col_name = 'amenity_childrens_books_and_toys'\n",
    "        elif amenity == r'children.*dinnerware':\n",
    "            col_name = 'amenity_childrens_dinnerware'\n",
    "        elif amenity == r'\\b(?<!paid\\s)(free\\s)?dryer\\b':\n",
    "            col_name = 'amenity_free_dryer'\n",
    "        elif amenity == r'(free(.*)parking|free(.*)garage)':\n",
    "            col_name = 'amenity_free_parking'\n",
    "        elif amenity == r'\\b(?<!paid\\s)(free\\s)?washer\\b':\n",
    "            col_name = 'amenity_free_washer'\n",
    "        elif amenity == r'paid(.*)parking':\n",
    "            col_name = 'amenity_paid_parking'\n",
    "        else:\n",
    "            col_name = 'amenity_' + amenity.replace(' ', '_')\n",
    "        cols_amenities.append(col_name)\n",
    "\n",
    "        # get set of synonyms for amenity\n",
    "        set_syn_amenities = get_noun_synonyms(amenity)\n",
    "\n",
    "        # iterate over df['amenities'], so that listing_amenities returns a string\n",
    "        for j, listing_amenities in enumerate(df['amenities']):\n",
    "            if pd.isnull(listing_amenities):\n",
    "                continue  # Skip to the next iteration if it's NaN\n",
    "\n",
    "            # Check if any of the items of set_syn_amenities occurs in listing_amenities\n",
    "            if any(re.search(item, listing_amenities, re.IGNORECASE) for item in set_syn_amenities):\n",
    "                # If so, assign 1 to the respective column and row\n",
    "                amenity_results[j, i] = 1\n",
    "\n",
    "    # Create DataFrame based on array amenity results with the column names cols_amenities\n",
    "    df_amenities = pd.DataFrame(amenity_results, columns=cols_amenities)\n",
    "    # Concat df and df_amenities\n",
    "    df = pd.concat([df, df_amenities], axis=1)\n",
    "\n",
    "    return df, cols_amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75fe0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_list)):\n",
    "    df = df_list[i]\n",
    "    df, cols_amenities = assign_amenity(df)\n",
    "\n",
    "    # Drop amenities column\n",
    "    df.drop(axis=1, columns='amenities', inplace=True)\n",
    "    df_list[i] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db3e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_list)):\n",
    "    amenities_per_listing = round(df_list[i][cols_amenities].sum().sum()/len(df_listings_tree))\n",
    "    print(f'{i}: On average {amenities_per_listing} amenities are attributed to one listing in Melbourne.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b39ecf",
   "metadata": {},
   "source": [
    "## Inspect Correlation Between Features\n",
    "The more variables used in a model, the more likely it is to overfit the training set. If the goal is to explain price, a sparse model with three features is more useful than a slightly better model with hundreds. So we keep only one of the highly correlated features. Let's look at the correlations first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f9c922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_list[1].corr()\n",
    "\n",
    "# Get the values that are larger than 0.8\n",
    "high_correlations = correlation_matrix[correlation_matrix > 0.6].stack().reset_index()\n",
    "high_correlations = high_correlations.rename(columns={'level_0': 'Feature 1', 'level_1': 'Feature 2', 0: 'Correlation'})\n",
    "\n",
    "# Filter out the diagonal elements and NaN values\n",
    "high_correlations = high_correlations[high_correlations['Feature 1'] != high_correlations['Feature 2']]\n",
    "high_correlations = high_correlations.dropna()\n",
    "\n",
    "# Drop duplicates\n",
    "high_correlations.drop_duplicates(subset='Correlation', inplace=True)\n",
    "\n",
    "# Sort by correlation in descending order\n",
    "high_correlations = high_correlations.sort_values(by='Correlation', ascending=False)\n",
    "high_correlations.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dc0d51",
   "metadata": {},
   "source": [
    "* The features *accommodates*, *beds* and *bedrooms* are highly correlated. Let's only keep *beds*.\n",
    "* Features concerning availability are highly correlated. Let's only keep *availability_90*.\n",
    "* Features concerning host_listings are highly correlated. Let's only keep *host_listings_count*.\n",
    "* Features regarding the minimum and maximum nights can be used to inspect effects on housing availability. In terms of price they should play a minor role. Let's not keep them either.\n",
    "* There are many features scoring the review of a listing and all are relatively correlated. Instead of keeping them all, lets create one feature that averages all the scores and drop the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab249d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_list)):\n",
    "    df = df_list[i]\n",
    "    \n",
    "    df.drop(['accommodates', 'bedrooms'], axis=1, inplace=True)\n",
    "    \n",
    "    # Get all the availability columns and remove the one to be kept\n",
    "    cols_availability = [col for col in df.columns if 'availability_' in col]\n",
    "    # Drop the irrelevant columns\n",
    "    cols_availability.remove('availability_90')\n",
    "    df.drop(cols_availability, axis=1, inplace=True)\n",
    "    \n",
    "    # Get all the host_listings columns and remove the one to be kept\n",
    "    cols_host_listings = [col for col in df.columns if 'listing' in col]\n",
    "    cols_host_listings.remove('host_listings_count')\n",
    "    # Drop the irrelevant columns\n",
    "    df.drop(cols_host_listings, axis=1, inplace=True)\n",
    "    \n",
    "    cols_minmax_nights = [col for col in df.columns if 'night' in col]\n",
    "    df.drop(cols_minmax_nights, axis=1, inplace=True)\n",
    "    \n",
    "    high_correlations[high_correlations['Feature 1'].str.contains('review_score', case=False)]\n",
    "\n",
    "    cols_review_scores = [col for col in df.columns if 'review_score' in col]\n",
    "    df['review_scores_mean'] = df.loc[:, cols_review_scores].mean(axis=1)\n",
    "    df.drop(cols_review_scores, axis=1, inplace=True)\n",
    "    \n",
    "    df_list[i] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ae06c4",
   "metadata": {},
   "source": [
    "# 5. Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae20301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "for i in range(len(df_list)):\n",
    "    df = df_list[i]\n",
    "    # Instantiate simple imputer\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    # Fit and transform\n",
    "    df = imp_mean.fit_transform(df)\n",
    "    # Recreate a dataframe from array\n",
    "    df = pd.DataFrame(df, columns=df_list[i].columns)\n",
    "    \n",
    "    df_list[i] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e4f74a",
   "metadata": {},
   "source": [
    "# 5. Train-Test-Split\n",
    "Split the data into training and testing sets to evaluate the model's performance on unseen data. This helps assess the model's generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce9da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_list)):\n",
    "    df = df_list[i]\n",
    "\n",
    "    # Put target column price as last\n",
    "    target = df['price']\n",
    "    df = df.drop('price', axis=1)\n",
    "    df['price'] = target\n",
    "    \n",
    "    # Check if target column is last\n",
    "    print(i, df.columns.get_loc('price'), len(df.columns))\n",
    "    \n",
    "    df_list[i] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81ab0c2",
   "metadata": {},
   "source": [
    "df_list[0] is the dataset with deleted missing values and df_list[1] is the dataset with imputed missing values. For the train_test_split I use the larger dataset with imputed means below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29591411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_list[1].iloc[:, :-1],  # Features (excluding the target column)\n",
    "                                                    df_list[1].iloc[:, -1],   # Target variable (last column)\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da94eca",
   "metadata": {},
   "source": [
    "# 6. Feature Scaling\n",
    "Normalize or standardize numerical features to bring them to a similar scale. This helps prevent features with larger values from dominating the model during training. Common scaling methods include min-max scaling (scaling to a specific range) and standardization (scaling to have zero mean and unit variance).\n",
    "### Kolmogorov-Smirnov Test for Normal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b0f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest\n",
    "\n",
    "normally_distributed_features = []\n",
    "not_normally_distributed_features = []\n",
    "\n",
    "# Iterate over each column in the dataframe\n",
    "for column in df_listings.columns:\n",
    "    # Perform Kolmogorov-Smirnov test on the column data\n",
    "    _, p_value = kstest(df_listings[column].dropna(), 'norm')\n",
    "    \n",
    "    # Set the significance level\n",
    "    alpha = 0.05\n",
    "    \n",
    "    # Check if the p-value is greater than the significance level\n",
    "    if p_value > alpha:\n",
    "        normally_distributed_features.append(column)\n",
    "    else:\n",
    "        not_normally_distributed_features.append(column)\n",
    "\n",
    "print(\"Normally Distributed Features:\")\n",
    "print(normally_distributed_features)\n",
    "\n",
    "df_listings = df_listings[not_normally_distributed_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c54e5",
   "metadata": {},
   "source": [
    "### Perform Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the X_train and X_test\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c195f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Define the data and file names\n",
    "data = [X_train_scaled, X_test_scaled, y_train, y_test]\n",
    "file_names = ['X_train_scaled', 'X_test_scaled', 'y_train', 'y_test']\n",
    "\n",
    "# Save the data as pickles\n",
    "for i, name in enumerate(file_names):\n",
    "    with open(f'{name}.pkl', 'wb') as file:\n",
    "        pickle.dump(data[i], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024b05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Define the file names\n",
    "file_names = ['X_train_scaled', 'X_test_scaled', 'y_train', 'y_test']\n",
    "\n",
    "# Load the pickles\n",
    "data = []\n",
    "for name in file_names:\n",
    "    with open(f'{name}.pkl', 'rb') as file:\n",
    "        data.append(pickle.load(file))\n",
    "\n",
    "# Assign the loaded data to variables\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a6ccb6",
   "metadata": {},
   "source": [
    "# 7. Model Selection and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2468041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge, ElasticNet, LinearRegression, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import numpy as np\n",
    "\n",
    "list_models = [LinearRegression(), RandomForestRegressor(), SVR(), XGBRegressor(), GradientBoostingRegressor(),\n",
    "               ElasticNet(), SGDRegressor(), BayesianRidge(), LGBMRegressor()]\n",
    "list_model_names = ['Linear Regression', 'Random Forest Regression', 'Support Vector Regression', 'XGB Regression',\n",
    "                    'Gradient Boosting Regression', 'Elastic Net', 'Stochastic Gradient Descent Regression',\n",
    "                    'Bayesian Ridge', 'Light GBM Regressor']\n",
    "score_dict = {model_name: {'rmse': None, 'mae': None} for model_name in list_model_names}\n",
    "\n",
    "for i, model in enumerate(list_models):\n",
    "    model_name = list_model_names[i]\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    score_dict[model_name]['rmse'] = rmse\n",
    "    score_dict[model_name]['mae'] = mae\n",
    "    \n",
    "    print(f'{model_name} RMSE: {rmse}')\n",
    "    print(f'{model_name} MAE: {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723026dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "# Determine the three lowest RMSE and corresponding models\n",
    "dict(heapq.nsmallest(3, rmse_dict.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50cc077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, mean_absolute_error\n",
    "\n",
    "top3_models = [LGBMRegressor(), XGBRegressor(), RandomForestRegressor()]\n",
    "top3_model_names = ['Light GBM Regressor', 'XGB Regression', 'Random Forest Regression']\n",
    "\n",
    "mae = make_scorer(mean_absolute_error)\n",
    "\n",
    "score_dict = {}\n",
    "\n",
    "for i, model in enumerate(top3_models):\n",
    "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator=model,\n",
    "                                                            X=X_train_scaled,\n",
    "                                                            y=y_train,\n",
    "                                                            train_sizes=train_sizes,\n",
    "                                                            cv=5,\n",
    "                                                            #scoring='neg_mean_squared_error',\n",
    "                                                            #scoring='neg_mean_absolute_error',\n",
    "                                                            scoring=mae,\n",
    "                                                            shuffle=True)\n",
    "\n",
    "    # Calculate the mean and standard deviation of the scores\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Convert the scores to positive values and compute RMSE\n",
    "#    train_scores = np.sqrt(-train_scores)\n",
    "#    test_scores = np.sqrt(-test_scores)\n",
    "\n",
    "    # Compute the mean and standard deviation of the scores\n",
    "#    train_mean = np.mean(train_scores, axis=1)\n",
    "#    train_std = np.std(train_scores, axis=1)\n",
    "#    test_mean = np.mean(test_scores, axis=1)\n",
    "#    test_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    score_dict[top3_model_names[i]] = {'train sizes': train_sizes,\n",
    "                                       'train mean': train_mean,\n",
    "                                       'test mean': test_mean,\n",
    "                                       'train std': train_std,\n",
    "                                       'test std': test_std}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c62fe3",
   "metadata": {},
   "source": [
    "## Plot Learning Curves\n",
    "Plotting learning curves provides various information such as bias-variance trade-off, model performance, overfitting and underfitting, and model convergence. Only the top 3 models with the lowest RMSE are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e6c555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,4), sharey=True)\n",
    "\n",
    "\n",
    "for i, model_name in enumerate(top3_model_names):\n",
    "    \n",
    "    train_sizes, train_mean, test_mean, train_std, test_std = [score_dict[model_name][score] for score in score_dict[model_name]]\n",
    "    \n",
    "    ax[i].plot(train_sizes, train_mean, label='Training MAE')\n",
    "    ax[i].plot(train_sizes, test_mean, label='Validation MAE')\n",
    "    ax[i].fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "    ax[i].fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "    ax[i].set_xlabel('Training Set Size')\n",
    "    ax[i].set_ylabel('MAE')\n",
    "    #ax[i].set_xticks(np.arange(0,15000, 1000))\n",
    "    ax[i].set_title(f'Learning Curve {top3_model_names[i]}', loc='left')\n",
    "    ax[i].legend(loc='best')\n",
    "    ax[i].grid(True)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d3392",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores_lgbm = pd.DataFrame()\n",
    "\n",
    "for key, item in score_dict['Light GBM Regressor'].items():\n",
    "    df_scores_lgbm[key] = item\n",
    "\n",
    "df_scores_lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e8ee8e",
   "metadata": {},
   "source": [
    "## Perform Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Train/test set generation\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_list[1].iloc[:, :-1],\n",
    "                                                    df_list[1].iloc[:, -1],\n",
    "                                                    train_size=3981,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# Scale train and test sets with StandardScaler\n",
    "X_train_std = StandardScaler().fit_transform(X_train)\n",
    "X_test_std = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "# Init the transformer\n",
    "rfe = RFE(estimator=LGBMRegressor(), n_features_to_select=20)\n",
    "\n",
    "# Fit to the training data\n",
    "_ = rfe.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cb3ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rfe = df_list[1].iloc[:, :-1].copy()\n",
    "X_train_rfe = X_train_rfe.loc[:, rfe.support_]\n",
    "\n",
    "X_train_rfe.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8993fe5a",
   "metadata": {},
   "source": [
    "## Perform Recursive Feature Elemination with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44198b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Train/test set generation\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_list[1].iloc[:, :-1],\n",
    "                                                    df_list[1].iloc[:, -1],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# Scale train and test sets with StandardScaler\n",
    "X_train_std = StandardScaler().fit_transform(X_train)\n",
    "X_test_std = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "# Init, fit\n",
    "rfecv = RFECV(\n",
    "    estimator=LGBMRegressor(),\n",
    "    min_features_to_select=5,\n",
    "    step=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "_ = rfecv.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58697bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_rfecv = df_list[1].columns[:-1][_.support_]\n",
    "df_list[1].loc[:, cols_rfecv].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b2fe3",
   "metadata": {},
   "source": [
    "## Tune LGBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a546ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgbm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    param_grid = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse', \n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 10000, step=100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 3000, step=20),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 200, 10000, step=100),\n",
    "        'max_bin': trial.suggest_int('max_bin', 200, 300),\n",
    "        'lambda_l1': trial.suggest_int('lambda_l1', 0, 100, step=5),\n",
    "        'lambda_l2': trial.suggest_int('lambda_l2', 0, 100, step=5),\n",
    "        'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0, 15),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.2, 0.95, step=0.1),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.2, 0.95, step=0.1)\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    cv_scores = np.empty(5)\n",
    "\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "                   \n",
    "        model = lgbm.LGBMRegressor(**param_grid)\n",
    "        \n",
    "        model.fit(X_train,\n",
    "                  y_train,\n",
    "                  eval_set=[(X_test, y_test)],\n",
    "                  eval_metric='rmse',\n",
    "                  callbacks=[LightGBMPruningCallback(trial, 'rmse'),\n",
    "                             lgbm.early_stopping(50)])\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "        cv_scores[idx] = rmse\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Regressor\")\n",
    "func = lambda trial: objective(trial,\n",
    "                               X_train_rfe, # X\n",
    "                               df_list[1].iloc[:, -1], # y\n",
    "                              )\n",
    "\n",
    "study.optimize(func, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f017f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb086069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_rfe, # X\n",
    "                                                    df_list[1].iloc[:, -1], # y\n",
    "                                                    train_size = 3981 )\n",
    "# Create the XGBoost model\n",
    "model = LGBMRegressor(**study.best_params)\n",
    "\n",
    "# Set the training set sizes for the learning curve\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Compute the learning curve scores\n",
    "train_sizes, train_scores, test_scores = learning_curve(model,\n",
    "                                                        X_train,\n",
    "                                                        y_train, \n",
    "                                                        train_sizes=train_sizes, cv=5,\n",
    "                                                        scoring='neg_mean_squared_error',\n",
    "                                                        shuffle=True)\n",
    "\n",
    "# Convert the scores to positive values and compute RMSE\n",
    "train_scores = np.sqrt(-train_scores)\n",
    "test_scores = np.sqrt(-test_scores)\n",
    "\n",
    "# Compute the mean and standard deviation of the scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19395019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training RMSE')\n",
    "plt.plot(train_sizes, test_mean, label='Validation RMSE')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('RMSE')\n",
    "#plt.yticks(np.arange(0,300, 25))\n",
    "#plt.xticks(np.arange(0,15000,1000))\n",
    "plt.title('Learning Curve Tuned LGBM Regressor', loc='left')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dab1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(train_sizes, train_mean, test_mean), columns=['train sizes', 'train mean', 'test mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aeb004",
   "metadata": {},
   "source": [
    "Best practice when tuning hyperparameters are:\n",
    "1. Tune n_estimators with default model\n",
    "2. Tune hyperparameters optuna with n_estimators fixed at a value from step 1. This way you will be tuning hyperparameters in equal conditions.\n",
    "3. Tune n_estimators once again as a final step on a model with tuned (other) hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgbm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    param_grid = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 10000, step=100),\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    cv_scores = np.empty(5)\n",
    "\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "                   \n",
    "        model = lgbm.LGBMRegressor(**param_grid)\n",
    "        \n",
    "        model.fit(X_train,\n",
    "                  y_train,\n",
    "                  eval_set=[(X_test, y_test)],\n",
    "                  eval_metric='rmse',\n",
    "                  callbacks=[LightGBMPruningCallback(trial, 'rmse'),\n",
    "                             lgbm.early_stopping(100)])\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "        cv_scores[idx] = rmse\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Regressor\")\n",
    "func = lambda trial: objective(trial,\n",
    "                               df_list[1].iloc[:, :-1], # X\n",
    "                               df_list[1].iloc[:, -1], # y\n",
    "                              )\n",
    "\n",
    "study.optimize(func, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa70cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e28031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgbm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    param_grid = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'n_estimators': 4700, # Set fixed now from previous study\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 3000, step=20),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 200, 10000, step=100),\n",
    "        'max_bin': trial.suggest_int('max_bin', 200, 300),\n",
    "        'lambda_l1': trial.suggest_int('lambda_l1', 0, 100, step=5),\n",
    "        'lambda_l2': trial.suggest_int('lambda_l2', 0, 100, step=5),\n",
    "        'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0, 15),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.2, 0.95, step=0.1),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.2, 0.95, step=0.1)\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    cv_scores = np.empty(5)\n",
    "\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "                   \n",
    "        model = lgbm.LGBMRegressor(**param_grid)\n",
    "        \n",
    "        model.fit(X_train,\n",
    "                  y_train,\n",
    "                  eval_set=[(X_test, y_test)],\n",
    "                  eval_metric='rmse',\n",
    "                  callbacks=[LightGBMPruningCallback(trial, 'rmse'),\n",
    "                             lgbm.early_stopping(100)])\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "        cv_scores[idx] = rmse\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Regressor\")\n",
    "func = lambda trial: objective(trial,\n",
    "                               df_list[1].iloc[:, :-1], # X\n",
    "                               df_list[1].iloc[:, -1], # y\n",
    "                              )\n",
    "\n",
    "study.optimize(func, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328f2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d0cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgbm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    \n",
    "    param_grid = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 3000, 5500, step=50),\n",
    "        'learning_rate': 0.19285282614506857,\n",
    "        'num_leaves': 1420,\n",
    "        'max_depth': 12,\n",
    "        'min_data_in_leaf': 200,\n",
    "        'max_bin': 216,\n",
    "        'lambda_l1': 65,\n",
    "        'lambda_l2': 10,\n",
    "        'min_gain_to_split': 8.42379574046222,\n",
    "        'bagging_fraction': 0.6000000000000001,\n",
    "        'feature_fraction': 0.8}\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    cv_scores = np.empty(5)\n",
    "\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "                   \n",
    "        model = lgbm.LGBMRegressor(**param_grid)\n",
    "        \n",
    "        model.fit(X_train,\n",
    "                  y_train,\n",
    "                  eval_set=[(X_test, y_test)],\n",
    "                  eval_metric='rmse',\n",
    "                  callbacks=[LightGBMPruningCallback(trial, 'rmse'),\n",
    "                             lgbm.early_stopping(100)])\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "        cv_scores[idx] = rmse\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Regressor\")\n",
    "func = lambda trial: objective(trial,\n",
    "                               df_list[1].iloc[:, :-1], # X\n",
    "                               df_list[1].iloc[:, -1], # y\n",
    "                              )\n",
    "\n",
    "study.optimize(func, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea70a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LGBMRegressor(**study.best_params)\n",
    "\n",
    "# Set the training set sizes for the learning curve\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Compute the learning curve scores\n",
    "train_sizes, train_scores, test_scores = learning_curve(model,\n",
    "                                                        X_train_scaled,\n",
    "                                                        y_train, \n",
    "                                                        train_sizes=train_sizes, cv=5,\n",
    "                                                        scoring='neg_mean_squared_error',\n",
    "                                                        shuffle=True)\n",
    "\n",
    "# Convert the scores to positive values and compute RMSE\n",
    "train_scores = np.sqrt(-train_scores)\n",
    "test_scores = np.sqrt(-test_scores)\n",
    "\n",
    "# Compute the mean and standard deviation of the scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d52a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b8893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "params = {'n_estimators': 5400,\n",
    "          'learning_rate': 0.19285282614506857,\n",
    "          'num_leaves': 1420,\n",
    "          'max_depth': 12,\n",
    "          'min_data_in_leaf': 200,\n",
    "          'max_bin': 216,\n",
    "          'lambda_l1': 65,\n",
    "          'lambda_l2': 10,\n",
    "          'min_gain_to_split': 8.42379574046222,\n",
    "          'bagging_fraction': 0.6000000000000001,\n",
    "          'feature_fraction': 0.8}\n",
    "\n",
    "# Create the XGBoost model\n",
    "model = LGBMRegressor(**params)\n",
    "\n",
    "# Set the training set sizes for the learning curve\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Compute the learning curve scores\n",
    "train_sizes, train_scores, test_scores = learning_curve(model,\n",
    "                                                        X_train_scaled,\n",
    "                                                        y_train, \n",
    "                                                        train_sizes=train_sizes, cv=5,\n",
    "                                                        scoring='neg_mean_squared_error',\n",
    "                                                        shuffle=True)\n",
    "\n",
    "# Convert the scores to positive values and compute RMSE\n",
    "train_scores = np.sqrt(-train_scores)\n",
    "test_scores = np.sqrt(-test_scores)\n",
    "\n",
    "# Compute the mean and standard deviation of the scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c47773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training RMSE')\n",
    "plt.plot(train_sizes, test_mean, label='Validation RMSE')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.yticks(np.arange(0,300, 25))\n",
    "plt.xticks(np.arange(0,15000,1000))\n",
    "plt.title('Learning Curve Tuned LGBM Regressor', loc='left')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9727740",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "rsme = np.sqrt(mean_squared_error(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed88f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830fb3e",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334f5496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Create an instance of the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a learning curve plot\n",
    "train_sizes, train_scores, test_scores = learning_curve(model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "train_rmse = np.sqrt(-train_scores.mean(axis=1))\n",
    "test_rmse = np.sqrt(-test_scores.mean(axis=1))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_rmse, 'o-', color='r', label='Training RMSE')\n",
    "plt.plot(train_sizes, test_rmse, 'o-', color='g', label='Validation RMSE')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Learning Curve Linear Regression', loc='left')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee74971",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d71d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create the RandomForestRegressor model\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# Set the training set sizes for the learning curve\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Generate the learning curve by calling the learning_curve function\n",
    "# It returns the training set sizes, training scores, and test scores\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    estimator=model,  # The random forest regressor to evaluate\n",
    "    X=X_train_scaled,  # Input features (independent variables) of the training dataset\n",
    "    y=y_train,  # Target variable (dependent variable) of the training dataset\n",
    "    train_sizes=train_sizes,  # Array of training set sizes to use\n",
    "    cv=5,  # Number of cross-validation folds or the cross-validation strategy\n",
    "    scoring='neg_mean_squared_error'  # Evaluation metric used for scoring\n",
    ")\n",
    "\n",
    "\n",
    "# Convert the scores to positive values and compute RMSE\n",
    "train_scores = np.sqrt(-train_scores)\n",
    "test_scores = np.sqrt(-test_scores)\n",
    "\n",
    "# Compute the mean and standard deviation of the scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training RMSE')\n",
    "plt.plot(train_sizes, test_mean, label='Validation RMSE')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Learning Curve Random Forest Regressor', loc='left')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a6af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e0f6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9586e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_cols = pd.DataFrame(model.feature_importances_, index=df_list[1].columns[:-1]).sort_values(by=0, ascending=False).head(20).index\n",
    "\n",
    "X_train_20, X_test_20, y_train, y_test = train_test_split(df_list[1].loc[:, top_20_cols],\n",
    "                                                          df_list[1].iloc[:,-1],\n",
    "                                                          test_size=0.3,\n",
    "                                                          random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61004d2",
   "metadata": {},
   "source": [
    "## Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c89b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming X contains the features and y contains the target variable\n",
    "\n",
    "# Define the SVM model\n",
    "model = SVR()\n",
    "\n",
    "# Define the number of training samples to use for the learning curve\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Calculate the learning curve scores\n",
    "train_sizes, train_scores, test_scores = learning_curve(model,\n",
    "                                                        X_train_scaled,\n",
    "                                                        y_train,\n",
    "                                                        train_sizes=train_sizes,\n",
    "                                                        cv=5,\n",
    "                                                        scoring='neg_mean_squared_error')\n",
    "\n",
    "# Convert the negative mean squared error scores to positive RMSE scores\n",
    "train_scores = np.sqrt(-train_scores)\n",
    "test_scores = np.sqrt(-test_scores)\n",
    "\n",
    "# Calculate the mean and standard deviation of the RMSE scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training RMSE', color='blue')\n",
    "plt.plot(train_sizes, test_mean, label='Validation RMSE', color='red')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='red')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Learning Curve Support Vector Regressor', loc='left')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmean_rmse = np.mean(train_scores, axis=1)\n",
    "min_rmse = np.min(train_scores)\n",
    "\n",
    "print(f'mean rmse: {vmean_rmse,4} \\n\\n'\n",
    "      f'min rmse: {round(min_rmse,4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2fdde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define the XGBoost model\n",
    "knn = KNeighborsRegressor(algorithm='brute')\n",
    "\n",
    "# Define the number of training samples to use for the learning curve\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Calculate the learning curve scores\n",
    "train_sizes, train_scores, test_scores = learning_curve(knn,\n",
    "                                                        X_train_scaled,\n",
    "                                                        y_train,\n",
    "                                                        train_sizes=train_sizes,\n",
    "                                                        cv=5,\n",
    "                                                        scoring='neg_mean_squared_error')\n",
    "\n",
    "# Convert the negative mean squared error scores to positive RMSE scores\n",
    "train_scores = np.sqrt(-train_scores)\n",
    "test_scores = np.sqrt(-test_scores)\n",
    "\n",
    "# Calculate the mean and standard deviation of the RMSE scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training RMSE', color='blue')\n",
    "plt.plot(train_sizes, test_mean, label='Validation RMSE', color='red')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='red')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Learning Curve KNN Regressor', loc='left')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d97cc",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f636b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the XGBoost model\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "# Define the number of training samples to use for the learning curve\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Calculate the learning curve scores\n",
    "train_sizes, train_scores, test_scores = learning_curve(model,\n",
    "                                                        X_train_scaled,\n",
    "                                                        y_train,\n",
    "                                                        train_sizes=train_sizes,\n",
    "                                                        cv=5,\n",
    "                                                        scoring='neg_mean_squared_error')\n",
    "\n",
    "# Convert the negative mean squared error scores to positive RMSE scores\n",
    "train_scores = np.sqrt(-train_scores)\n",
    "test_scores = np.sqrt(-test_scores)\n",
    "\n",
    "# Calculate the mean and standard deviation of the RMSE scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training RMSE', color='blue')\n",
    "plt.plot(train_sizes, test_mean, label='Validation RMSE', color='red')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='red')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Learning Curve XGBoost Regressor', loc='left')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a3307",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmean_rmse = np.mean(test_scores, axis=1)\n",
    "min_rmse = np.min(test_scores)\n",
    "\n",
    "print(f'mean rmse: {vmean_rmse,4} \\n\\n'\n",
    "      f'min rmse: {round(min_rmse,4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729acbc4",
   "metadata": {},
   "source": [
    "Although XGBoost produces the lowest RMSE of the tested models, the learning curve above indicates overfitting, which can be a problem for tree-based algorithms. Let's tune the model with *optuna* to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f80304a",
   "metadata": {},
   "source": [
    "## Model Tuning\n",
    "Model tuning, also known as hyperparameter optimisation, plays a critical role in refining machine learning models by systematically adjusting the model's parameters and hyperparameters to maximise its performance and improve its ability to generalise well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the search space for hyperparameters\n",
    "    param = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'num_boost_round': 100000, # Fix the boosting round and use early stopping\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 5),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 10.0),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 10.0),\n",
    "        'lambda': trial.suggest_float('lambda', 0.1, 10.0),\n",
    "        'alpha': trial.suggest_float('alpha', 0.0, 10.0),\n",
    "        \n",
    "#        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 1),  # L1 regularization parameter (Lasso)\n",
    "        'reg_lambda': trial.suggest_float('reg_alpha', 0.001, 1), # L2 regularization parameter (Ridge)\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # Split the data into further training and validation sets (three sets are preferable)\n",
    "    train_data, valid_data, train_target, valid_target = train_test_split(df_list[1].iloc[:, :-1],\n",
    "                                                                          df_list[1].iloc[:, -1],\n",
    "                                                                          test_size=0.2,\n",
    "                                                                          random_state=42)\n",
    "    \n",
    "    # Convert the data into DMatrix format\n",
    "    dtrain = xgb.DMatrix(train_data, label=train_target)\n",
    "    dvalid = xgb.DMatrix(valid_data, label=valid_target)\n",
    "    \n",
    "    # Define the pruning callback for early stopping\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, 'validation-rmse')\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    model = xgb.train(param, dtrain, evals=[(dvalid, 'validation')], early_stopping_rounds=100, callbacks=[pruning_callback])\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    dtest = xgb.DMatrix(valid_data)\n",
    "    y_pred = model.predict(dtest)\n",
    "    \n",
    "    # Calculate the root mean squared error\n",
    "    rmse = mean_squared_error(valid_target, y_pred, squared=False)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=1000) # Control the number of trials\n",
    "\n",
    "# Print the best hyperparameters and the best RMSE\n",
    "best_params = study.best_params\n",
    "best_rmse = study.best_value\n",
    "print(\"Best Hyperparameters: \", best_params)\n",
    "print(\"Best RMSE: \", best_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a546a768",
   "metadata": {},
   "source": [
    "## Rerun XGBoost With Tuned Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create the XGBoost model\n",
    "model = xgb.XGBRegressor(**best_params)\n",
    "\n",
    "# Set the training set sizes for the learning curve\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Compute the learning curve scores\n",
    "train_sizes, train_scores, test_scores = learning_curve(model,\n",
    "                                                        X_train_scaled,\n",
    "                                                        y_train, \n",
    "                                                        train_sizes=train_sizes, cv=5,\n",
    "                                                        scoring='neg_mean_squared_error',\n",
    "                                                        shuffle=True)\n",
    "\n",
    "# Convert the scores to positive values and compute RMSE\n",
    "train_scores = np.sqrt(-train_scores)\n",
    "test_scores = np.sqrt(-test_scores)\n",
    "\n",
    "# Compute the mean and standard deviation of the scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c9f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training RMSE')\n",
    "plt.plot(train_sizes, test_mean, label='Validation RMSE')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.yticks(np.arange(0,275,25))\n",
    "plt.xticks(np.arange(0,15000,1000))\n",
    "plt.ylim(0,275)\n",
    "plt.title('Learning Curve XGBoost Tuned')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d0fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training RMSE')\n",
    "plt.plot(train_sizes, test_mean, label='Validation RMSE')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Relative RMSE')\n",
    "plt.yticks(np.arange(0,.475,0.05))\n",
    "plt.xticks(np.arange(0,15000,1000))\n",
    "plt.ylim(0,.475)\n",
    "plt.title('Learning Curve XGBoost Tuned')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c4fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_train_scaled = imp_mean.fit_transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb40efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=df_list[1].columns[:-1])\n",
    "\n",
    "# Init, fit\n",
    "rfecv = RFECV(\n",
    "    estimator=XGBRegressor(**best_params),\n",
    "    min_features_to_select=5,\n",
    "    step=25,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "_ = rfecv.fit(X_train_scaled,\n",
    "              y_train.apply(lambda x: np.log(x+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e66bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.loc[:, rfecv.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e89f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_rcef = _.transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a884f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the XGBoost model\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "# Define the number of training samples to use for the learning curve\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Calculate the learning curve scores\n",
    "train_sizes, train_scores, test_scores = learning_curve(model,\n",
    "                                                        X_train_scaled.loc[:, rfecv.support_],\n",
    "                                                        y_train.apply(lambda x: np.log(x+1)),\n",
    "                                                        train_sizes=train_sizes,\n",
    "                                                        cv=5,\n",
    "                                                        scoring='neg_mean_squared_error')\n",
    "\n",
    "# Convert the negative mean squared error scores to positive RMSE scores\n",
    "train_scores = np.sqrt(-train_scores)\n",
    "test_scores = np.sqrt(-test_scores)\n",
    "\n",
    "# Calculate the mean and standard deviation of the RMSE scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe1fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training RMSE')\n",
    "plt.plot(train_sizes, test_mean, label='Validation RMSE')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Relative RMSE')\n",
    "plt.yticks(np.arange(0,.5,0.05))\n",
    "plt.xticks(np.arange(0,15000,1000))\n",
    "plt.ylim(0,.5)\n",
    "plt.title('Learning Curve XGBoost Tuned')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea5df0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_rcef.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36073a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the search space for hyperparameters\n",
    "    param = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'num_boost_round': 100000, # Fix the boosting round and use early stopping\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 10.0),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 10.0),\n",
    "        'lambda': trial.suggest_float('lambda', 0.1, 10.0),\n",
    "        'alpha': trial.suggest_float('alpha', 0.0, 10.0)}\n",
    "    \n",
    "    # Split the data into further training and validation sets (three sets are preferable)\n",
    "    train_data, valid_data, train_target, valid_target = train_test_split(X_train_scaled_rcef, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Convert the data into DMatrix format\n",
    "    dtrain = xgb.DMatrix(train_data, label=train_target)\n",
    "    dvalid = xgb.DMatrix(valid_data, label=valid_target)\n",
    "    \n",
    "    # Define the pruning callback for early stopping\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, 'validation-rmse')\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    model = xgb.train(param, dtrain, evals=[(dvalid, 'validation')], early_stopping_rounds=100, callbacks=[pruning_callback])\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    dtest = xgb.DMatrix(valid_data)\n",
    "    y_pred = model.predict(dtest)\n",
    "    \n",
    "    # Calculate the root mean squared error\n",
    "    rmse = mean_squared_error(valid_target, y_pred, squared=False)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100) # Control the number of trials\n",
    "\n",
    "# Print the best hyperparameters and the best RMSE\n",
    "best_params = study.best_params\n",
    "best_rmse = study.best_value\n",
    "print(\"Best Hyperparameters: \", best_params)\n",
    "print(\"Best RMSE: \", best_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b76051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create the XGBoost model\n",
    "model = xgb.XGBRegressor(**best_params)\n",
    "\n",
    "# Set the training set sizes for the learning curve\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Compute the learning curve scores\n",
    "train_sizes, train_scores, test_scores = learning_curve(model, X_train_scaled_rcef, y_train, train_sizes=train_sizes, cv=5,\n",
    "                                                        scoring='neg_mean_squared_error', shuffle=True)\n",
    "\n",
    "# Convert the scores to positive values and compute RMSE\n",
    "train_scores = np.sqrt(-train_scores)\n",
    "test_scores = np.sqrt(-test_scores)\n",
    "\n",
    "# Compute the mean and standard deviation of the scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training RMSE')\n",
    "plt.plot(train_sizes, test_mean, label='Validation RMSE')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Learning Curve XGBoost Tuned')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea222f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Create the Lasso model\n",
    "lasso_model = Lasso(alpha=1.0)\n",
    "\n",
    "# Fit the model to the data\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the coefficients after regularization\n",
    "coefficients = lasso_model.coef_\n",
    "\n",
    "coefficients_df = pd.DataFrame(index=df_list[0].columns[:-1], data={'L1_coefficients':coefficients})\n",
    "coefficients_df = coefficients_df.sort_values(by='L1_coefficients', ascending=False)\n",
    "coefficients_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb4dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_L1 = coefficients_df.loc[coefficients_df.L1_coefficients>0].index\n",
    "df_listings_L1 = df_listings.loc[:, col_L1]\n",
    "df_listings_L1.loc[:, 'price'] = target\n",
    "\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(df_listings_L1.iloc[:, :-1],\n",
    "                                                                  df_listings_L1.iloc[:,-1], \n",
    "                                                                  train_size=0.8, test_size=0.2,\n",
    "                                                                  random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd339cfe",
   "metadata": {},
   "source": [
    "There are in general two ways that you can control overfitting in XGBoost:\n",
    "\n",
    "    The first way is to directly control model complexity.\n",
    "\n",
    "        This includes max_depth, min_child_weight and gamma.\n",
    "\n",
    "    The second way is to add randomness to make training robust to noise.\n",
    "\n",
    "        This includes subsample and colsample_bytree.\n",
    "\n",
    "        You can also reduce stepsize eta. Remember to increase num_round when you do so.\n",
    "\n",
    "\n",
    "The learning curve above shows that the performance of the model is significantly improved by tuning. The training RMSE decreases up to a training set size of approximately 2,250, avoiding overfitting up to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa5eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#Establish CV scheme\n",
    "CV = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "ix_training, ix_test = [], []\n",
    "# Loop through each fold and append the training & test indices to the empty lists above\n",
    "for fold in CV.split(df_listings):\n",
    "    ix_training.append(fold[0]), ix_test.append(fold[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38192776",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_listings.iloc[:, :-1], df_listings.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dbbb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "SHAP_values_per_fold = []\n",
    "## Loop through each outer fold and extract SHAP values \n",
    "for i, (train_outer_ix, test_outer_ix) in enumerate(zip(ix_training, ix_test)):\n",
    "    #Verbose\n",
    "    print('\\n------ Fold Number:',i)\n",
    "    X_train, X_test = X.iloc[train_outer_ix, :], X.iloc[test_outer_ix, :]\n",
    "    y_train, y_test = y.iloc[train_outer_ix], y.iloc[test_outer_ix]\n",
    "\n",
    "    model = xgb.XGBRegressor(random_state=42) # Random state for reproducibility (same results every time)\n",
    "    fit = model.fit(X_train, y_train)\n",
    "    y_pred = fit.predict(X_test)\n",
    "    result = mean_squared_error(y_test, y_pred)\n",
    "    print('RMSE:',round(np.sqrt(result),4))\n",
    "\n",
    "    # Use SHAP to explain predictions\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    for SHAPs in shap_values:\n",
    "        SHAP_values_per_fold.append(SHAPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35801496",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAP_values_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a4f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index = [ix for ix_test_fold in ix_test for ix in ix_test_fold]\n",
    "shap.summary_plot(np.array(SHAP_values_per_fold), X.reindex(new_index), max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a1cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) # Reproducibility \n",
    "CV_repeats = 10\n",
    "# Make a list of random integers between 0 and 10000 of length = CV_repeats to act as different data splits\n",
    "random_states = np.random.randint(10000, size=CV_repeats) \n",
    "\n",
    "######## Use a dict to track the SHAP values of each observation per CV repitition \n",
    "\n",
    "shap_values_per_cv = dict()\n",
    "for sample in X.index:\n",
    "    ## Create keys for each sample\n",
    "    shap_values_per_cv[sample] = {} \n",
    "    ## Then, keys for each CV fold within each sample\n",
    "    for CV_repeat in range(CV_repeats):\n",
    "        shap_values_per_cv[sample][CV_repeat] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbe0e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30bc2a52",
   "metadata": {},
   "source": [
    "## Top 10 Features Based on Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a046e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c335efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use params from Optuna model tuning\n",
    "params = {'eta': 0.29957346988768996, 'max_depth': 5, 'subsample': 0.7620788683085897, 'colsample_bytree': 0.5445127771106928, 'gamma': 2.7717979772086045, 'min_child_weight': 0.852634656050349, 'lambda': 5.390432625416122, 'alpha': 0.9850402641809253}\n",
    "\n",
    "# Fit the XGB model\n",
    "model = xgb.XGBRegressor(**params)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Create DataFrame with importances\n",
    "df_importance = pd.DataFrame(importances, index=df_listings.columns[:-1], columns=['importances'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815c5a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 features according to their importances\n",
    "df_importance.sort_values(by='importances', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eaa286",
   "metadata": {},
   "source": [
    "### Plot Top 10 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b1ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_x = 10\n",
    "\n",
    "# Define the official Airbnb colors\n",
    "colors = ['#FF5A5F', '#00A699']\n",
    "\n",
    "# Create a plot\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "# Filter and sort the DataFrame\n",
    "df_plot = df_importance.sort_values('importances', ascending=False)\n",
    "df_plot = df_plot[:top_x]\n",
    "\n",
    "# Reverse the order of the DataFrame\n",
    "df_plot = df_plot.iloc[::-1]\n",
    "    \n",
    "# Customize y_labels\n",
    "y_labels = [idx.replace('_', ' ').title().replace('Of', 'of').replace('Amenity', '') for idx in df_plot.index]\n",
    "\n",
    "# Create a horizontal bar plot\n",
    "ax.barh(y_labels, df_plot['importances'] * 100, height=0.8, color=colors[0])\n",
    "ax.set_xlabel('Relative importance in %')\n",
    "ax.set_title(f'Top Feature Importances', loc='left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Top10_FeatureImportances.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbdad44",
   "metadata": {},
   "source": [
    "### Plot Top 10 of Reviews and Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2a898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Create DataFrame with importances\n",
    "df_importance = pd.DataFrame(importances, index=df_listings.columns[:-1], columns=['importances'])\n",
    "\n",
    "feature_groups = ['review', 'amenity']\n",
    "top_x = 10\n",
    "\n",
    "# Define the official Airbnb colors\n",
    "colors = ['#FF5A5F', '#00A699']\n",
    "\n",
    "# Create a plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for i, feature in enumerate(feature_groups):\n",
    "    # Filter and sort the DataFrame\n",
    "    df_plot = df_importance.filter(axis=0, like=feature).sort_values('importances', ascending=False)\n",
    "    \n",
    "    # Get only top_x features\n",
    "    df_plot = df_plot[:top_x]\n",
    "    \n",
    "    # Reverse the order of the DataFrame\n",
    "    df_plot = df_plot.iloc[::-1]\n",
    "    \n",
    "    # Customize y_labels\n",
    "    y_labels = [idx.replace('_', ' ').title().replace('Of', 'of').replace('Amenity', '') for idx in df_plot.index]\n",
    "\n",
    "    # Create a horizontal bar plot\n",
    "    ax[i].barh(y_labels, df_plot['importances'] * 100, height=0.8, color=colors[i])\n",
    "    ax[i].set_xlabel('Relative importance in %')\n",
    "    ax[i].set_title(f'Top {top_x} {feature.title()} Importance', loc='left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Top10Importance_Review_Amenity.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814f2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_review = df_importance.filter(like='review', axis=0).sum()\n",
    "tot_amenity = df_importance.filter(like='amenity', axis=0).sum()\n",
    "print(f'Sum of review contribution: {round(tot_review.item()*100,2)}% \\n'\n",
    "      f'Sum of amenity contribution: {round(tot_amenity.item()*100,2)}%')\n",
    "\n",
    "len(df_importance.filter(like='amenity', axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aad88ec",
   "metadata": {},
   "source": [
    "## SHAP-Plots for feature importance\n",
    "SHAP (SHapley Additive exPlanations) values are a method used to explain the output of a machine learning model by assigning importance values to each input feature. These values quantify the contribution of each feature to the final prediction made by the model.\n",
    "\n",
    "SHAP values provide a way to understand and quantify the impact of each feature on the predictions made by a machine learning model. They help to understand which features are driving the predictions and provide insight into the underlying factors influencing the model's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d162a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84d902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Use params from optuna tuning\n",
    "params = {'eta': 0.26678249922705494, 'max_depth': 9, 'subsample': 0.8827268140624525, 'colsample_bytree': 0.9770252347812884, 'gamma': 3.0813302070917885, 'min_child_weight': 9.1582640029387, 'lambda': 2.0019794387256966, 'alpha': 2.762623241915627}\n",
    "\n",
    "# Fit the XGB model\n",
    "model = xgb.XGBRegressor(**params)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Initialize shap with initjs()\n",
    "shap.initjs()\n",
    "\n",
    "# Create an XGB explainer object\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Calculate SHAP values for all features\n",
    "shap_values = explainer(X_test_scaled)\n",
    "\n",
    "# Add feature names\n",
    "shap_values.feature_names_ = df_listings.iloc[:, :-1].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859daf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to long calculation time of shap_values save a pickle for later use\n",
    "import pickle\n",
    "\n",
    "# Save shap_values as a pickle file\n",
    "with open('shap_values.pkl', 'wb') as f:\n",
    "    pickle.dump(shap_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a1ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load shap_values from the pickle file\n",
    "with open('shap_values.pkl', 'rb') as f:\n",
    "    shap_values = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d52645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename features for plotting\n",
    "shap_values.feature_names = [feature.replace('_',' ').replace('amenity', 'amenity:').title() for feature in shap_values.feature_names_]\n",
    "\n",
    "# Plot SHAP beeswarm\n",
    "shap.plots.beeswarm(shap_values, color=cmap, show=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('SHAP_Summary_General.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de24c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Create dataframe with SHAP-values\n",
    "df_shap_values = pd.DataFrame(data=shap_values.abs.mean(0).values,\n",
    "                              index=shap_values.feature_names)\n",
    "# Rename column\n",
    "df_shap_values.rename(columns={0:'shap_values'}, inplace=True)\n",
    "\n",
    "df_shap_values.sort_values(by='shap_values', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e14a98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for all numeric review columns\n",
    "top_10 = df_shap_values.sort_values(by='shap_values', ascending=False).head(10).index\n",
    "top_10 = [item.replace(' ','_').replace('Amenity:', 'Amenity').lower() for item in top_10]\n",
    "\n",
    "corr_matrix = df_listings[top_10].corr()\n",
    "sn.heatmap(corr_matrix, cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da480b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11003d89",
   "metadata": {},
   "source": [
    "## Create Summary Plots for Top10 Amenities and Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2563cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap_values.values[:,idx].shape\n",
    "\n",
    "X_test_scaled[:, idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d5161",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_types = ['Review', 'Amenity']\n",
    "\n",
    "for i, feature_type in enumerate(feature_types):\n",
    "    idx = [shap_values.feature_names.index(item) for item in shap_values.feature_names if feature_type in item]\n",
    "    \n",
    "    shap.summary_plot(shap_values.values[:,idx],\n",
    "                      X_test_scaled[:, idx],\n",
    "                      max_display=10,\n",
    "                      show=False,\n",
    "                      cmap=cmap,\n",
    "                      feature_names=[col.replace('_', ' ').replace('Amenity: ', '').title().strip() for col in shap_values.feature_names if feature_type in col])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'SHAP_Summary_{feature_type.title()}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fba06ba",
   "metadata": {},
   "source": [
    "## Create 3 Barplots For Random Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97aa167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Select two random indices\n",
    "random_indices = random.sample(range(shap_values.shape[0]), k=3)\n",
    "\n",
    "# Default SHAP colors\n",
    "default_pos_color = \"#ff0051\"\n",
    "default_neg_color = \"#008bfb\"\n",
    "\n",
    "# Define the official Airbnb colors\n",
    "positive_color = '#FF5A5F'\n",
    "negative_color = '#00A699'\n",
    "\n",
    "fig = plt.figure()\n",
    "for i, idx in enumerate(random_indices):\n",
    "    ax = fig.add_subplot(int(f'13{i+1}'))\n",
    "    shap.plots.bar(shap_values[idx], show=False)\n",
    "    \n",
    "    # Change the colormap of the artists\n",
    "    for fc in plt.gcf().get_children():\n",
    "        # Ignore last Rectangle\n",
    "        for fcc in fc.get_children()[:-1]:\n",
    "            if isinstance(fcc, matplotlib.patches.Rectangle):\n",
    "                if matplotlib.colors.to_hex(fcc.get_facecolor()) == default_pos_color:\n",
    "                    fcc.set_facecolor(positive_color)\n",
    "                elif matplotlib.colors.to_hex(fcc.get_facecolor()) == default_neg_color:\n",
    "                    fcc.set_facecolor(negative_color)\n",
    "            elif isinstance(fcc, plt.Text):\n",
    "                if matplotlib.colors.to_hex(fcc.get_color()) == default_pos_color:\n",
    "                    fcc.set_color(positive_color)\n",
    "                elif matplotlib.colors.to_hex(fcc.get_color()) == default_neg_color:\n",
    "                    fcc.set_color(negative_color)\n",
    "\n",
    "plt.gcf().set_size_inches(20, 6)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Barplots_3_Listings.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05136141",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da41f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we convert back to the original data\n",
    "# (note we can do this because X_std is a set of univariate transformations of X)\n",
    "shap_values.data = scaler.inverse_transform(shap_values.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74c2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your Jupyter notebook with initjs(), otherwise you will get an error message.\n",
    "shap.initjs()\n",
    "\n",
    "# Custom colors\n",
    "positive_color = \"#ca0020\"\n",
    "negative_color = \"#92c5de\"\n",
    "\n",
    "shap.force_plot(shap_values[0], \n",
    "                plot_cmap = [positive_color, negative_color])\n",
    "\n",
    "# Write in a function\n",
    "def shap_plot(j):\n",
    "    explainerModel = shap.TreeExplainer(model)\n",
    "    shap_values_Model = explainerModel.shap_values(X_train_scaled)\n",
    "    p = shap.force_plot(explainerModel.expected_value,\n",
    "                        shap_values_Model[j],\n",
    "                        X_train.iloc[[j]], plot_cmap=[positive_color, negative_color])\n",
    "    return(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc4f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot(155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5618c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put target column price as last\n",
    "target = df_listings['price']\n",
    "df_listings = df_listings.drop('price', axis=1)\n",
    "df_listings['price'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd392413",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c379b2a",
   "metadata": {},
   "source": [
    "## Create a Waterfall Plot for a Listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6dda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Default SHAP colors\n",
    "default_pos_color = \"#ff0051\"\n",
    "default_neg_color = \"#008bfb\"\n",
    "\n",
    "# Define the official Airbnb colors\n",
    "positive_color = '#FF5A5F'\n",
    "negative_color = '#00A699'\n",
    "\n",
    "# Plot Waterfall Plot\n",
    "shap.plots.waterfall(shap_values[0], show = False)\n",
    "# Change the colormap of the artists\n",
    "for fc in plt.gcf().get_children():\n",
    "    for fcc in fc.get_children():\n",
    "        if (isinstance(fcc, matplotlib.patches.FancyArrow)):\n",
    "            if (matplotlib.colors.to_hex(fcc.get_facecolor()) == default_pos_color):\n",
    "                fcc.set_facecolor(positive_color)\n",
    "            elif (matplotlib.colors.to_hex(fcc.get_facecolor()) == default_neg_color):\n",
    "                fcc.set_color(negative_color)\n",
    "        elif (isinstance(fcc, plt.Text)):\n",
    "            if (matplotlib.colors.to_hex(fcc.get_color()) == default_pos_color):\n",
    "                fcc.set_color(positive_color)\n",
    "            elif (matplotlib.colors.to_hex(fcc.get_color()) == default_neg_color):\n",
    "                fcc.set_color(negative_color)\n",
    "\n",
    "plt.tight_layout()                \n",
    "plt.savefig('WaterfallPlotListing.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc67cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import xgboost as xgb\n",
    "\n",
    "# Use params from optuna tuning\n",
    "params = {'eta': 0.26678249922705494, 'max_depth': 9, 'subsample': 0.8827268140624525, 'colsample_bytree': 0.9770252347812884, 'gamma': 3.0813302070917885, 'min_child_weight': 9.1582640029387, 'lambda': 2.0019794387256966, 'alpha': 2.762623241915627}\n",
    "\n",
    "# Fit the XGB model\n",
    "model = xgb.XGBRegressor(**params)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Initialize shap with initjs()\n",
    "shap.initjs()\n",
    "\n",
    "# Create an XGB explainer object\n",
    "explainer = shap.explainers.Tree(model, X_test_scaled)\n",
    "#explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Calculate SHAP values for all features\n",
    "shap_values = explainer(X_test_scaled)\n",
    "\n",
    "# Add feature names\n",
    "shap_values.feature_names_ = df_listings.iloc[:, :-1].columns\n",
    "\n",
    "shap_values.data = scaler.inverse_transform(shap_values.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f3a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(base_value=explainer.expected_value,  # The expected model output\n",
    "                shap_values=shap_values[14],  # SHAP values for the instance to explain\n",
    "                features=df_listings.iloc[[j],:-1],  # Features of the instance to explain\n",
    "                plot_cmap=[positive_color, negative_color]  # Color map for positive and negative contributions\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573677e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546372f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AirBnB_conda",
   "language": "python",
   "name": "airbnb_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
